[
  {
    "objectID": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#massachusetts-weighted-regression-model",
    "href": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#massachusetts-weighted-regression-model",
    "title": "Polishing and Final Exploration",
    "section": "Massachusetts Weighted Regression Model",
    "text": "Massachusetts Weighted Regression Model\n\n\n\nCall:\nlm(formula = mn_score_all ~ perecd + povertyall + unempall + \n    lninc50all + baplusall + perblk + perasn + perwht, data = selected_variables_MA, \n    weights = weights)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-2.5187 -0.3788 -0.0423  0.3591  3.8483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.74363    0.22682   3.279  0.00105 ** \nperecd      -0.83297    0.03873 -21.506  &lt; 2e-16 ***\npovertyall  -0.75291    0.12242  -6.150 8.36e-10 ***\nunempall     2.16004    0.13713  15.752  &lt; 2e-16 ***\nlninc50all  -0.06063    0.01899  -3.192  0.00142 ** \nbaplusall    0.92817    0.02638  35.191  &lt; 2e-16 ***\nperblk      -0.15714    0.06596  -2.382  0.01724 *  \nperasn       0.93507    0.06239  14.987  &lt; 2e-16 ***\nperwht       0.12889    0.04194   3.073  0.00213 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.634 on 4766 degrees of freedom\nMultiple R-squared:  0.7659,    Adjusted R-squared:  0.7655 \nF-statistic:  1949 on 8 and 4766 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation for MA Weighted Regression Model\n\n\n\nInterpretations of Weighted Regression Model Coefficients for Massachusetts\n\n\n\n\n\n\n\nTerm\nCoefficient Estimate\nInterpretation\n\n\n\n\nIntercept\n0.74363\nBaseline mean score for all students, when all predictors are at zero.\n\n\nperecd\n-0.83297\nFor each percentage point increase in economically disadvantaged students, there is an expected decrease of approximately 0.833 in the mean score.\n\n\npovertyall\n-0.75291\nFor each percentage point increase in poverty rate, there is an expected decrease of approximately 0.753 in the mean score.\n\n\nunempall\n2.16004\nFor each percentage point increase in unemployment rate, there is an unexpected increase of approximately 2.160 in the mean score.\n\n\nlninc50all\n-0.06063\nA 1% increase in median income is associated with a decrease of 0.061 in the mean score, indicating a complex relationship with other socioeconomic factors.\n\n\nbaplusall\n0.92817\nFor each percentage point increase in the percentage of parents with a bachelor’s degree or higher, there is an expected increase of approximately 0.928 in the mean score.\n\n\nperblk\n-0.15714\nFor each percentage point increase in the percentage of Black students, there is an expected decrease of approximately 0.157 in the mean score.\n\n\nperasn\n0.93507\nFor each percentage point increase in the percentage of Asian students, there is an expected increase of approximately 0.935 in the mean score.\n\n\nperwht\n0.12889\nFor each percentage point increase in the percentage of White students, there is an expected but smaller increase of approximately 0.129 in the mean score.\n\n\nR-squared\n0.76590\nThe model explains approximately 76.59% of the variance in the mean score for all students in the dataset.\n\n\n\n\n\n\n\nMA Weighted Regression Model Residual Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#mississippi-weighted-regression-model",
    "href": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#mississippi-weighted-regression-model",
    "title": "Polishing and Final Exploration",
    "section": "Mississippi Weighted Regression Model",
    "text": "Mississippi Weighted Regression Model\n\n\n\nCall:\nlm(formula = mn_score_all ~ perecd + povertyall + unempall + \n    lninc50all + baplusall + perblk + perasn + perwht, data = selected_variables_MS, \n    weights = weights)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-1.25271 -0.25875 -0.01196  0.23318  1.78759 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.31107    0.39065   5.916 3.70e-09 ***\nperecd      -0.22535    0.03649  -6.175 7.57e-10 ***\npovertyall  -0.75443    0.11960  -6.308 3.27e-10 ***\nunempall    -0.72945    0.16229  -4.495 7.24e-06 ***\nlninc50all  -0.19933    0.03475  -5.736 1.07e-08 ***\nbaplusall    0.93576    0.06519  14.355  &lt; 2e-16 ***\nperblk      -0.56360    0.07357  -7.661 2.52e-14 ***\nperasn       3.80540    0.46435   8.195 3.75e-16 ***\nperwht      -0.05968    0.07638  -0.781    0.435    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3893 on 2827 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6743 \nF-statistic: 734.7 on 8 and 2827 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation for MS Weighted Regression Model\n\n\n\nInterpretations of Weighted Regression Model Coefficients for Mississippi\n\n\n\n\n\n\n\nTerm\nCoefficient Estimate\nInterpretation\n\n\n\n\nIntercept\n2.31107\nBaseline mean score for all students, when all predictors are at zero.\n\n\nperecd\n-0.22535\nFor each percentage point increase in economically disadvantaged students, there is an expected decrease of approximately 0.225 in the mean score.\n\n\npovertyall\n-0.75443\nFor each percentage point increase in poverty rate, there is an expected decrease of approximately 0.754 in the mean score.\n\n\nunempall\n-0.72945\nFor each percentage point increase in unemployment rate, there is an expected decrease of approximately 0.729 in the mean score.\n\n\nlninc50all\n-0.19933\nA 1% increase in median income is associated with a decrease of 0.199 in the mean score, indicating a complex relationship with other socioeconomic factors.\n\n\nbaplusall\n0.93576\nFor each percentage point increase in the percentage of parents with a bachelor’s degree or higher, there is an expected increase of approximately 0.936 in the mean score.\n\n\nperblk\n-0.56360\nFor each percentage point increase in the percentage of Black students, there is an expected decrease of approximately 0.564 in the mean score.\n\n\nperasn\n3.80540\nFor each percentage point increase in the percentage of Asian students, there is an expected increase of approximately 3.805 in the mean score.\n\n\nperwht\n-0.05968\nFor each percentage point increase in the proportion of White students, there is an expected decrease of approximately 0.060 in the mean score, although this effect is not statistically significant (p-value: 0.435).\n\n\nR-squared\n0.67520\nThe model explains approximately 67.52% of the variance in the mean score for all students in the dataset.\n\n\n\n\n\n\n\nMS Weighted Regression Model Residual Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nResiduals vs. Fitted Values Plot:\nThe values on this plot appear to be randomly distributed around the zero line, indicating that the assumption of constant variance (homoscedasticity) and linearity is satisfied.\n\n\nNormal Q-Q Plot of Residuals:\nThe points on the Normal Q-Q Plot fall approximately along the diagonal line, suggesting that the residuals are normally distributed.\n\n\nResiduals vs. Predictor Variables Plot:\nThere is no discernible pattern in the Residuals vs. Predictor Variables Plot, indicating that the model adequately captures the relationship between the predictor variables (perecd, povertyall, unempall, lninc50all, baplusall, perblk, perasn, perwht) and the response variable (mn_score_all).\n\n\nCook’s Distance Plot:\nAlmost all points on the Cook’s Distance Plot are clustered near the bottom of the plot along one line, indicating that most observations have low influence on the regression coefficients. However, there are some points that are not aligned with the main cluster, suggesting they might have a higher influence on the model.\n\n\nScale-Location Plot:\nThe line on the Scale-Location Plot is slightly positively directed, indicating a possible violation of the homoscedasticity assumption. However, the points are randomly spread, which suggests that the assumption holds reasonably well overall."
  },
  {
    "objectID": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#massachusetts-robust-regression-model",
    "href": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#massachusetts-robust-regression-model",
    "title": "Polishing and Final Exploration",
    "section": "Massachusetts Robust Regression Model",
    "text": "Massachusetts Robust Regression Model\n\n\n\nCall: rlm(formula = mn_score_all ~ perecd + povertyall + unempall + \n    lninc50all + baplusall + perblk + perasn + perwht, data = selected_variables_MA)\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.686284 -0.091326 -0.005956  0.095575  0.906861 \n\nCoefficients:\n            Value    Std. Error t value \n(Intercept)   0.6881   0.2257     3.0493\nperecd       -0.8944   0.0334   -26.7885\npovertyall   -0.7035   0.1054    -6.6759\nunempall      2.1884   0.1325    16.5164\nlninc50all   -0.0418   0.0190    -2.2032\nbaplusall     0.8714   0.0257    33.8958\nperblk       -0.3181   0.0457    -6.9610\nperasn        0.7202   0.0541    13.3028\nperwht       -0.0039   0.0288    -0.1344\n\nResidual standard error: 0.1381 on 4766 degrees of freedom\n\n\nPseudo R-squared:  0.7696934 \n\n\n\nInterpretation for MA Robust Regression Model\n\n\n\nInterpretations of the Robust Regression Model Coefficients\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\n(Intercept)\n0.6881\nBaseline mean score for all students on the logged scale when all predictors are at zero.\n\n\nperecd\n-0.8944\nEach percentage point increase in economically disadvantaged students is associated with a 0.8944 point decrease in mean score.\n\n\npovertyall\n-0.7035\nEach percentage point increase in poverty rate is associated with a 0.7035 point decrease in mean score.\n\n\nunempall\n2.1884\nEach percentage point increase in unemployment rate is associated with a 2.1884 point increase in mean score, which is counterintuitive.\n\n\nlninc50all\n-0.0418\nA 1% increase in median income (not logged) is associated with a 0.0418 point decrease in mean score, which may indicate the presence of other interacting variables.\n\n\nbaplusall\n0.8714\nEach percentage point increase in the proportion of parents with a bachelor’s degree is associated with a 0.8714 point increase in mean score.\n\n\nperblk\n-0.3181\nEach percentage point increase in the proportion of Black students is associated with a 0.3181 point decrease in mean score.\n\n\nperasn\n0.7202\nEach percentage point increase in the proportion of Asian students is associated with a 0.7202 point increase in mean score.\n\n\nperwht\n-0.0039\nEach percentage point increase in the proportion of White students is associated with a negligible change in mean score.\n\n\nR-squared\n0.7697\nThe model explains approximately 76.97% of the variance in the mean score for all students in the dataset."
  },
  {
    "objectID": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#mississippi-robust-regression-model",
    "href": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#mississippi-robust-regression-model",
    "title": "Polishing and Final Exploration",
    "section": "Mississippi Robust Regression Model",
    "text": "Mississippi Robust Regression Model\n\n\n\nCall: rlm(formula = mn_score_all ~ perecd + povertyall + unempall + \n    lninc50all + baplusall + perblk + perasn + perwht, data = selected_variables_MS)\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.513638 -0.115744 -0.002019  0.110301  0.741378 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept)  2.3299  0.3966     5.8746\nperecd      -0.2273  0.0377    -6.0212\npovertyall  -0.7626  0.1188    -6.4197\nunempall    -0.6635  0.1555    -4.2662\nlninc50all  -0.2040  0.0353    -5.7726\nbaplusall    0.9211  0.0677    13.6061\nperblk      -0.5384  0.0729    -7.3860\nperasn       4.3782  0.5150     8.5013\nperwht      -0.0387  0.0762    -0.5085\n\nResidual standard error: 0.1679 on 2827 degrees of freedom\n\n\nPseudo R-squared:  0.6624828 \n\n\n\nInterpretation for MS Robust Regression Model\n\n\n\nInterpretations of the Robust Regression Model Coefficients for Mississippi\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\nIntercept\n2.3299\nBaseline mean score for all students on the logged scale when all predictors are at zero.\n\n\nperecd\n-0.2273\nEach percentage point increase in economically disadvantaged students is associated with a 0.2273 point decrease in mean score.\n\n\npovertyall\n-0.7626\nEach percentage point increase in poverty rate is associated with a 0.7626 point decrease in mean score.\n\n\nunempall\n-0.6635\nEach percentage point increase in unemployment rate is associated with a 0.6635 point decrease in mean score.\n\n\nlninc50all\n-0.2040\nA 1% increase in median income (on the natural log scale) is associated with a 0.2040 point decrease in mean score.\n\n\nbaplusall\n0.9211\nEach percentage point increase in the proportion of parents with a bachelor’s degree is associated with a 0.9211 point increase in mean score.\n\n\nperblk\n-0.5384\nEach percentage point increase in the proportion of Black students is associated with a 0.5384 point decrease in mean score.\n\n\nperasn\n4.3782\nEach percentage point increase in the proportion of Asian students is associated with a 4.3782 point increase in mean score.\n\n\nperwht\n-0.0387\nEach percentage point increase in the proportion of White students is associated with a 0.0387 point decrease in mean score.\n\n\nR-squared\n0.6625\nThe model explains approximately 66.25% of the variance in the mean score for all students in the dataset."
  },
  {
    "objectID": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#polishing-the-graphs",
    "href": "posts/2024-04-24-blog-7-polishing/blog-7-polishing.html#polishing-the-graphs",
    "title": "Polishing and Final Exploration",
    "section": "Polishing the Graphs",
    "text": "Polishing the Graphs\nTo polish off the graphs that were made as part of the blog posts to prepare them for the main pages, we changed the labels of the variables and also the colors to make them stand out against each other. Hex codes for specific colors were used to make sure that the graphs looked exactly how we wanted them to look. The variables were redefined on the facet labels, but not in the dataframe itself so that previous plotting and calculations would not need to be edited as well.\nMost of the widgets in the provided links will not work for us, as our data does not fit these types of models."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#our-trends",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#our-trends",
    "title": "Exploring the Data",
    "section": "Our Trends",
    "text": "Our Trends\nAs seen in blog post 3, race was a predictor variable in the scores seen in students. Economic status is another. We want to see how these two variables work together, rather than individually like we did last week."
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#more-exploration",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#more-exploration",
    "title": "Exploring the Data",
    "section": "More Exploration",
    "text": "More Exploration\nOur regression model predicts the average test score (mn_score_all) based on subgroup-specific test scores, including those for Asian (mn_score_asn), Black (mn_score_blk), economically challenged (mn_score_ecd), female (mn_score_fem), Hispanic (mn_score_hsp), male (mn_score_male), multiracial (mn_score_mtr), and white (mn_score_wht) students. Each coefficient (β) represents the expected change in the average test score for a one-unit change in the corresponding subgroup’s test score. We anticipate identifying significant associations between subgroup-specific performance and the overall average test score. Our model’s goodness of fit will be evaluated using metrics such as R-squared and the F-statistic’s significance, while residual plots will ensure adherence to model assumptions."
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#linear-regression-model",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#linear-regression-model",
    "title": "Exploring the Data",
    "section": "Linear regression model",
    "text": "Linear regression model\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncleaned_seda &lt;- read_rds(\"dataset/cleaned_seda.rds\")\n\nstate_dummies &lt;- model.matrix(~ state - 1, data = cleaned_seda)\n\ncleaned_seda &lt;- cbind(cleaned_seda, state_dummies)\n\nscore_regression_model &lt;- lm(mn_score_all ~ lninc50all + baplusall + povertyall + numstu_ecd + numstu_male + numstu_fem + perasn + perhsp + perblk + perwht + state_dummies, data = cleaned_seda)\n\nsummary(score_regression_model)\n\n\nCall:\nlm(formula = mn_score_all ~ lninc50all + baplusall + povertyall + \n    numstu_ecd + numstu_male + numstu_fem + perasn + perhsp + \n    perblk + perwht + state_dummies, data = cleaned_seda)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6397 -0.1241  0.0000  0.1249  1.5668 \n\nCoefficients: (2 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -2.176e+00  4.353e-02 -50.004  &lt; 2e-16 ***\nlninc50all            1.470e-01  3.815e-03  38.517  &lt; 2e-16 ***\nbaplusall             1.071e+00  6.135e-03 174.575  &lt; 2e-16 ***\npovertyall           -3.971e-01  1.569e-02 -25.305  &lt; 2e-16 ***\nnumstu_ecd           -1.323e-05  2.486e-06  -5.322 1.03e-07 ***\nnumstu_male          -5.309e-04  4.241e-05 -12.519  &lt; 2e-16 ***\nnumstu_fem            5.647e-04  4.509e-05  12.524  &lt; 2e-16 ***\nperasn                1.064e+00  1.289e-02  82.529  &lt; 2e-16 ***\nperhsp                2.657e-01  6.063e-03  43.821  &lt; 2e-16 ***\nperblk               -7.117e-02  6.497e-03 -10.953  &lt; 2e-16 ***\nperwht                6.279e-01  5.792e-03 108.416  &lt; 2e-16 ***\nstate_dummiesstateAK -2.720e-01  1.288e-02 -21.110  &lt; 2e-16 ***\nstate_dummiesstateAL -1.304e-01  9.685e-03 -13.460  &lt; 2e-16 ***\nstate_dummiesstateAR -1.060e-01  9.158e-03 -11.580  &lt; 2e-16 ***\nstate_dummiesstateAZ -1.946e-01  9.648e-03 -20.173  &lt; 2e-16 ***\nstate_dummiesstateCA -3.310e-01  8.891e-03 -37.235  &lt; 2e-16 ***\nstate_dummiesstateCO -9.809e-02  1.016e-02  -9.650  &lt; 2e-16 ***\nstate_dummiesstateCT -1.169e-01  9.500e-03 -12.310  &lt; 2e-16 ***\nstate_dummiesstateDC -4.583e-01  4.853e-02  -9.444  &lt; 2e-16 ***\nstate_dummiesstateDE -5.912e-03  1.450e-02  -0.408   0.6834    \nstate_dummiesstateFL  7.671e-02  1.044e-02   7.347 2.03e-13 ***\nstate_dummiesstateGA  1.118e-03  9.375e-03   0.119   0.9051    \nstate_dummiesstateHI -4.973e-01  4.904e-02 -10.142  &lt; 2e-16 ***\nstate_dummiesstateIA -9.138e-02  8.991e-03 -10.163  &lt; 2e-16 ***\nstate_dummiesstateID -1.370e-01  1.002e-02 -13.671  &lt; 2e-16 ***\nstate_dummiesstateIL -1.537e-01  8.784e-03 -17.496  &lt; 2e-16 ***\nstate_dummiesstateIN  2.176e-02  9.026e-03   2.411   0.0159 *  \nstate_dummiesstateKS -3.955e-02  9.141e-03  -4.326 1.52e-05 ***\nstate_dummiesstateKY -4.425e-02  9.365e-03  -4.725 2.30e-06 ***\nstate_dummiesstateLA -8.232e-02  1.048e-02  -7.854 4.05e-15 ***\nstate_dummiesstateMA -1.433e-02  9.145e-03  -1.567   0.1171    \nstate_dummiesstateMD -4.257e-02  1.333e-02  -3.194   0.0014 ** \nstate_dummiesstateME -1.443e-01  9.547e-03 -15.115  &lt; 2e-16 ***\nstate_dummiesstateMI -2.183e-01  8.857e-03 -24.648  &lt; 2e-16 ***\nstate_dummiesstateMN -5.288e-04  8.987e-03  -0.059   0.9531    \nstate_dummiesstateMO -8.483e-02  8.916e-03  -9.514  &lt; 2e-16 ***\nstate_dummiesstateMS -2.163e-02  9.640e-03  -2.243   0.0249 *  \nstate_dummiesstateMT -7.646e-02  9.882e-03  -7.738 1.02e-14 ***\nstate_dummiesstateNC  6.782e-02  9.762e-03   6.947 3.74e-12 ***\nstate_dummiesstateND -7.413e-02  9.971e-03  -7.434 1.06e-13 ***\nstate_dummiesstateNE -6.230e-02  9.335e-03  -6.675 2.49e-11 ***\nstate_dummiesstateNH -7.846e-02  9.726e-03  -8.067 7.24e-16 ***\nstate_dummiesstateNJ -4.397e-02  8.966e-03  -4.904 9.40e-07 ***\nstate_dummiesstateNM -1.841e-01  1.074e-02 -17.147  &lt; 2e-16 ***\nstate_dummiesstateNV -2.154e-01  1.584e-02 -13.593  &lt; 2e-16 ***\nstate_dummiesstateNY -1.845e-01  8.946e-03 -20.622  &lt; 2e-16 ***\nstate_dummiesstateOH  9.188e-04  8.838e-03   0.104   0.9172    \nstate_dummiesstateOK -7.812e-02  8.949e-03  -8.729  &lt; 2e-16 ***\nstate_dummiesstateOR -2.845e-01  9.522e-03 -29.883  &lt; 2e-16 ***\nstate_dummiesstatePA -1.659e-02  8.859e-03  -1.872   0.0612 .  \nstate_dummiesstatePR         NA         NA      NA       NA    \nstate_dummiesstateRI -1.471e-01  1.176e-02 -12.511  &lt; 2e-16 ***\nstate_dummiesstateSC -7.981e-02  1.014e-02  -7.870 3.56e-15 ***\nstate_dummiesstateSD -9.893e-02  9.775e-03 -10.121  &lt; 2e-16 ***\nstate_dummiesstateTN -1.157e-01  9.588e-03 -12.064  &lt; 2e-16 ***\nstate_dummiesstateTX -6.315e-02  8.780e-03  -7.192 6.40e-13 ***\nstate_dummiesstateUT -1.191e-01  1.145e-02 -10.403  &lt; 2e-16 ***\nstate_dummiesstateVA  1.732e-02  9.597e-03   1.804   0.0712 .  \nstate_dummiesstateVT -1.398e-01  1.170e-02 -11.945  &lt; 2e-16 ***\nstate_dummiesstateWA -1.740e-01  9.462e-03 -18.392  &lt; 2e-16 ***\nstate_dummiesstateWI -4.996e-02  8.913e-03  -5.605 2.08e-08 ***\nstate_dummiesstateWV -2.919e-01  1.075e-02 -27.153  &lt; 2e-16 ***\nstate_dummiesstateWY         NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2073 on 204281 degrees of freedom\n  (28436 observations deleted due to missingness)\nMultiple R-squared:  0.6625,    Adjusted R-squared:  0.6625 \nF-statistic:  6685 on 60 and 204281 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary output of the regression model demonstrates an exceptional fit to the data, with a remarkably low residual standard error of 0.006255, indicating minimal unexplained variability in the dependent variable. The multiple R-squared value of 0.9997 suggests that nearly all of the variance in the dependent variable is accounted for by the independent variables included in the model, while the adjusted R-squared value confirms this finding, adjusting for the number of predictors. The F-statistic is impressively large at 4.84e+06, with an associated p-value of less than 2.2e-16, providing strong evidence of the model’s overall significance. However, it’s noteworthy that not all coefficient estimates are statistically significant at the 5% level, suggesting that while the model as a whole is highly significant, individual predictors may not contribute significantly to explaining the variability in the dependent variable. Despite this, the model still offers valuable insights into the relationship between subgroup-specific test scores and overall average test performance."
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#residuals-and-related-statistics-table",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#residuals-and-related-statistics-table",
    "title": "Exploring the Data",
    "section": "Residuals and Related Statistics Table:",
    "text": "Residuals and Related Statistics Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\nmn_score_all\nlninc50all\nbaplusall\npovertyall\nnumstu_ecd\nnumstu_male\nnumstu_fem\nperasn\nperhsp\nperblk\nperwht\nstate_dummies\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n0.1371997\n10.97552\n0.1799880\n0.1002362\n11.50000\n15.83333\n15.83333\n0.0048587\n0.0087912\n0.0128562\n0.9734938\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0.0488767\n10.97552\n0.1799880\n0.1002362\n11.50000\n15.83333\n15.83333\n0.0048587\n0.0087912\n0.0128562\n0.9734938\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.1122827\n10.88742\n0.1677282\n0.1015800\n12.83333\n15.16667\n18.00000\n0.0042735\n0.0115492\n0.0248317\n0.9593456\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n-0.1143257\n10.88742\n0.1677282\n0.1015800\n12.66667\n15.16667\n17.83333\n0.0042735\n0.0115492\n0.0248317\n0.9593456\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n-0.0378648\n10.88924\n0.1896547\n0.1186809\n13.16667\n16.66667\n19.00000\n0.0000000\n0.0146428\n0.0189163\n0.9664410\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n-0.2208493\n10.88924\n0.1896547\n0.1186809\n13.16667\n16.50000\n19.16667\n0.0000000\n0.0146428\n0.0189163\n0.9664410\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#residual-plot",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#residual-plot",
    "title": "Exploring the Data",
    "section": "Residual Plot",
    "text": "Residual Plot\n\n\n\n\n\nThe Residuals vs. Fitted plot signals an excellent fit of the regression model to the data. This close alignment indicates minimal discrepancies between predicted and observed values, while the consistent spread of residuals suggests uniform variability across different levels of the independent variables. With no discernible patterns, the model accurately captures underlying relationships without systematic errors, meeting key assumptions of linear regression. However, it’s worth noting the presence of some visible leverage points in the data."
  },
  {
    "objectID": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#summary-tables",
    "href": "posts/2024-04-08-blog-4-exploring-the-data/blog-4-exploring-the-data.html#summary-tables",
    "title": "Exploring the Data",
    "section": "Summary Tables",
    "text": "Summary Tables\n\n\n   district              year         state             subject         \n Length:232778      Min.   :2009   Length:232778      Length:232778     \n Class :character   1st Qu.:2011   Class :character   Class :character  \n Mode  :character   Median :2013   Mode  :character   Mode  :character  \n                    Mean   :2013                                        \n                    3rd Qu.:2016                                        \n                    Max.   :2018                                        \n                                                                        \n  mn_score_all      numstu_all        mn_score_asn      numstu_asn      \n Min.   :-3.735   Min.   :    1.00   Min.   :-2.92    Min.   :    1.00  \n 1st Qu.:-0.197   1st Qu.:   35.17   1st Qu.: 0.26    1st Qu.:    2.00  \n Median : 0.030   Median :   92.67   Median : 0.60    Median :    3.83  \n Mean   : 0.025   Mean   :  290.29   Mean   : 0.59    Mean   :   29.22  \n 3rd Qu.: 0.249   3rd Qu.:  237.67   3rd Qu.: 0.93    3rd Qu.:   14.00  \n Max.   : 2.865   Max.   :74519.00   Max.   : 2.79    Max.   :11901.33  \n NA's   :23236    NA's   :14         NA's   :205143   NA's   :120352    \n  mn_score_blk      numstu_blk        mn_score_ecd     numstu_ecd     \n Min.   :-3.40    Min.   :    1.00   Min.   :-3.55   Min.   :    1.0  \n 1st Qu.:-0.63    1st Qu.:    2.25   1st Qu.:-0.42   1st Qu.:   15.0  \n Median :-0.45    Median :    6.33   Median :-0.24   Median :   38.5  \n Mean   :-0.44    Mean   :   75.86   Mean   :-0.26   Mean   :  157.6  \n 3rd Qu.:-0.25    3rd Qu.:   35.17   3rd Qu.:-0.08   3rd Qu.:  105.2  \n Max.   : 1.16    Max.   :21703.67   Max.   : 1.24   Max.   :61479.3  \n NA's   :180838   NA's   :92526      NA's   :61961   NA's   :5949     \n  mn_score_fem     numstu_fem        mn_score_hsp      numstu_hsp      \n Min.   :-2.33   Min.   :    1.00   Min.   :-2.36    Min.   :    1.00  \n 1st Qu.:-0.13   1st Qu.:   17.50   1st Qu.:-0.47    1st Qu.:    2.67  \n Median : 0.10   Median :   45.67   Median :-0.28    Median :    7.20  \n Mean   : 0.10   Mean   :  142.73   Mean   :-0.26    Mean   :   87.57  \n 3rd Qu.: 0.33   3rd Qu.:  116.67   3rd Qu.:-0.07    3rd Qu.:   31.00  \n Max.   : 1.99   Max.   :36852.67   Max.   : 1.29    Max.   :38164.75  \n NA's   :51659   NA's   :2566       NA's   :163843   NA's   :45723     \n mn_score_male    numstu_male        mn_score_mtr      numstu_mtr     \n Min.   :-3.63   Min.   :    1.00   Min.   :-1.99    Min.   :   1.00  \n 1st Qu.:-0.26   1st Qu.:   18.50   1st Qu.:-0.17    1st Qu.:   2.00  \n Median :-0.03   Median :   47.83   Median : 0.05    Median :   4.00  \n Mean   :-0.03   Mean   :  149.32   Mean   : 0.08    Mean   :  13.03  \n 3rd Qu.: 0.21   3rd Qu.:  122.50   3rd Qu.: 0.30    3rd Qu.:  10.17  \n Max.   : 1.88   Max.   :37666.33   Max.   : 1.75    Max.   :1904.00  \n NA's   :49416   NA's   :2009       NA's   :208467   NA's   :107273   \n   numstu_nec        mn_score_wht     numstu_wht           pernam        \n Min.   :    1.00   Min.   :-1.96   Min.   :    1.00   Min.   :0.000000  \n 1st Qu.:   16.33   1st Qu.:-0.05   1st Qu.:   23.33   1st Qu.:0.000000  \n Median :   45.67   Median : 0.14   Median :   64.17   Median :0.002276  \n Mean   :  140.02   Mean   : 0.15   Mean   :  147.44   Mean   :0.030590  \n 3rd Qu.:  123.33   3rd Qu.: 0.35   3rd Qu.:  155.83   3rd Qu.:0.007697  \n Max.   :19079.33   Max.   : 1.88   Max.   :11594.33   Max.   :1.000000  \n NA's   :10751      NA's   :42400   NA's   :3927                         \n     perasn              perhsp            perblk             perwht      \n Min.   :0.0000000   Min.   :0.00000   Min.   :0.000000   Min.   :0.0000  \n 1st Qu.:0.0002838   1st Qu.:0.01723   1st Qu.:0.004059   1st Qu.:0.5940  \n Median :0.0059829   Median :0.04773   Median :0.014154   Median :0.8552  \n Mean   :0.0200438   Mean   :0.13654   Mean   :0.076034   Mean   :0.7368  \n 3rd Qu.:0.0169583   3rd Qu.:0.15032   3rd Qu.:0.052669   3rd Qu.:0.9494  \n Max.   :1.0000000   Max.   :1.00000   Max.   :1.000000   Max.   :1.0000  \n                                                                          \n     perfl              perecd          perell          perspeced     \n Min.   :0.001393   Min.   :0.002   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.246167   1st Qu.:0.343   1st Qu.:0.00000   1st Qu.:0.1082  \n Median :0.386980   Median :0.500   Median :0.00917   Median :0.1356  \n Mean   :0.405481   Mean   :0.502   Mean   :0.04232   Mean   :0.1381  \n 3rd Qu.:0.545425   3rd Qu.:0.654   3rd Qu.:0.04186   3rd Qu.:0.1655  \n Max.   :0.994910   Max.   :1.000   Max.   :1.00000   Max.   :1.2639  \n                    NA's   :3296    NA's   :168       NA's   :168     \n    totenrl           lninc50all       baplusall       povertyall   \n Min.   :    0.80   Min.   : 9.421   Min.   :0.001   Min.   :0.000  \n 1st Qu.:   35.50   1st Qu.:10.626   1st Qu.:0.142   1st Qu.:0.087  \n Median :   93.33   Median :10.802   Median :0.190   Median :0.125  \n Mean   :  293.98   Mean   :10.838   Mean   :0.228   Mean   :0.131  \n 3rd Qu.:  240.33   3rd Qu.:11.012   3rd Qu.:0.272   3rd Qu.:0.165  \n Max.   :74619.83   Max.   :12.506   Max.   :0.889   Max.   :0.483  \n                    NA's   :6323     NA's   :6323    NA's   :6323   \n    unempall        snapall      single_momall  \n Min.   :0.000   Min.   :0.000   Min.   :0.002  \n 1st Qu.:0.054   1st Qu.:0.057   1st Qu.:0.109  \n Median :0.070   Median :0.096   Median :0.142  \n Mean   :0.072   Mean   :0.105   Mean   :0.151  \n 3rd Qu.:0.087   3rd Qu.:0.142   3rd Qu.:0.181  \n Max.   :0.357   Max.   :0.524   Max.   :0.615  \n NA's   :6323    NA's   :6323    NA's   :6323"
  },
  {
    "objectID": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#our-data",
    "href": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#our-data",
    "title": "Equity and Data Cleaning",
    "section": "Our data",
    "text": "Our data\nWe cleaned our data before importing, as it was 500mB. We kept the scores for each race and the locations, as that is what we are most interested in."
  },
  {
    "objectID": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#preliminary-tables",
    "href": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#preliminary-tables",
    "title": "Equity and Data Cleaning",
    "section": "Preliminary Tables",
    "text": "Preliminary Tables\nTo begin, we wanted to make a summary table of general descriptibe statistics for the races included in our data as well as the economically challenged students to see the variation between their test scores. This is not broken down by location yet but we wanted to better visualize the data so we would know how to proceed.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n  mn_score_all     mn_score_asn     mn_score_blk     mn_score_ecd  \n Min.   :-3.735   Min.   :-2.92    Min.   :-3.40    Min.   :-3.55  \n 1st Qu.:-0.197   1st Qu.: 0.26    1st Qu.:-0.63    1st Qu.:-0.42  \n Median : 0.030   Median : 0.60    Median :-0.45    Median :-0.24  \n Mean   : 0.025   Mean   : 0.59    Mean   :-0.44    Mean   :-0.26  \n 3rd Qu.: 0.249   3rd Qu.: 0.93    3rd Qu.:-0.25    3rd Qu.:-0.08  \n Max.   : 2.865   Max.   : 2.79    Max.   : 1.16    Max.   : 1.24  \n NA's   :23236    NA's   :205143   NA's   :180838   NA's   :61961"
  },
  {
    "objectID": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#plots",
    "href": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#plots",
    "title": "Equity and Data Cleaning",
    "section": "Plots",
    "text": "Plots\nOur next step for Data Cleaning process is to analyze the histograms for selected variables.By examining these histograms, we can visually analyze the distribution of test scores within each subgroup. This visualization helps in understanding the spread, central tendency, and potential outliers within the test scores across different racial groups and economic statuses.\n\n\n\n\n\nAdditionally, we need to look at the boxplots of selected variables for the Data Cleaning process. The boxplots of test scores across racial and economic groups reveal outliers, differences in central tendency and spread, skewness or symmetry, disparities between groups, and missing values. These observations guide data cleaning efforts to ensure the reliability of subsequent analyses.\n\n\nWarning: Removed 471178 rows containing non-finite values (`stat_boxplot()`)."
  },
  {
    "objectID": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#data-equity",
    "href": "posts/2024-04-01-blog-post-3-equity-and-data-cleaning/blog-post-3-equity-and-data-cleaning.html#data-equity",
    "title": "Equity and Data Cleaning",
    "section": "Data Equity",
    "text": "Data Equity\nThe Stanford Education Data Archive (SEDA) presents an opportunity to use educational data to benefit all students, especially those from underprivileged communities. Researchers and policymakers can use SEDA as a helpful tool for social justice in education by upholding the principles of beneficence and justice.\nIt’s important to prioritize research addressing educational disparities. Our research involves conducting analyses highlighting gaps and identifying actional resource allocation. By focusing on the needs of underserved communities, such research can lead to policy recommendations that impact students’ educational experiences and outcomes. Researchers must prioritize analyses that offer tangible solutions to underserved communities, ensuring that the insights gained do not disproportionately benefit advantaged groups.\nWe need to ensure that research findings are accessible to a wide range of stakeholders, including educators, policymakers, and community members, to enable informed decision-making across all levels of the education system. Restricting access to sensitive information protects the privacy and security of individuals represented in the data."
  },
  {
    "objectID": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-1-stanford-education-data-archive",
    "href": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-1-stanford-education-data-archive",
    "title": "Possible data sets",
    "section": "Option 1: Stanford Education Data Archive",
    "text": "Option 1: Stanford Education Data Archive\nhttps://edopportunity.org/get-the-data/seda-archive-downloads/\nSEDA is a long-term project working to standardize state test scores in order to make them comparable between states. As each state implements their own statewide standardized tests for each grade level with different scoring scales, it is normally quite challenging to perform a direct comparison of differences between states. SEDA aims to resolve that, with available data on individual school performance all the way up to statewide performance, covering math and reading tests for multiple grade levels. We are thinking we would use district-level data, as that would allow for a lot of detailed work if we choose to focus on a specific region. They also have an extensive file of different covariates for each district with percentages of different racial groups, gender, poverty levels, and other interesting variables. The files are easily accessible and well-documented, with the most recent dataset spanning from 2009-2018 and detailed codebooks available for each geographic grouping level. Their methodology is explained on their website. We downloaded the seda_cov_geodist_long_4.1 and seda_geodist_long_gcs_4.1 files. The initial datafiles are quite large (~700,000 and ~1,200,000 observations, respectively), so even though they are very nicely organized and polished, we were not able to upload them to our repository without some compressing. We were able to collapse each file by year and district. Instead of having an observation for six or seven different grade levels for each year for each district, there was only one observation for each district per year, taking the mean for each selected column across all grades. We also removed some of the variable columns that seemed less relevant to what we were thinking about for potential questions–mostly related to how certain covariates like poverty and race interact and how they might affect test scores. With these edits, we were able to get the files to a size that could be easily shared while still containing huge amounts of relevant information for any potential education inequality related project we might choose to do."
  },
  {
    "objectID": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-2-us-police-shooting-dataset",
    "href": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-2-us-police-shooting-dataset",
    "title": "Possible data sets",
    "section": "Option 2: US police shooting dataset",
    "text": "Option 2: US police shooting dataset\nhttps://www.kaggle.com/datasets/ahsen1330/us-police-shootings?resource=download\nThe dataset was collected from Kaggle.com, and the dataset comprises records of police shooting incidents in the United States, including a total of 4896 rows, each representing an individual case, and 15 columns, which document various attributes associated with each incident, such as the date, manner of death, and demographics of the individual involved (including age, gender, and race), etc. Moreover, we can clean the dataset by organizing variables, such as the manner of death and types of arms, to numerical data and filter out the rows that contain missing values. In exploring this data, the primary goal is to examine and analyze racial disparities and criminal justice within the context of police shootings. Anticipated challenges include navigating through potential biases in data reporting and incomplete information about individual cases."
  },
  {
    "objectID": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-3-health-insurance-coverage-in-the-us-in-2022",
    "href": "posts/2024-03-04-blog-post-1-possible-data-sets/blog-post-1-possible-data-sets.html#option-3-health-insurance-coverage-in-the-us-in-2022",
    "title": "Possible data sets",
    "section": "Option 3: Health Insurance Coverage in the US in 2022",
    "text": "Option 3: Health Insurance Coverage in the US in 2022\nhttps://www.census.gov/library/publications/2023/demo/p60-281.html\nThe dataset “Health Insurance Coverage in the United States: 2022,” sourced from the United States Census Bureau, offers detailed insights into the health insurance landscape across the nation for the year 2022. It includes information on various aspects of health insurance coverage, such as the number of insured individuals, coverage rates, types of insurance (e.g., private, public), demographic breakdowns, work experience, education level, income-to-poverty ratio, and incomes per household. We can utilize this dataset to conduct analysis aimed at understanding patterns of health insurance coverage, identifying populations with high rates of uninsurance, assessing the impact of policy changes on coverage rates, and exploring disparities in access to health care services. Moreover, we can investigate the relationship between health insurance coverage and health outcomes, healthcare utilization patterns, and financial burden due to medical expenses. One challenge to using this data is the amount of cleaning necessary to get this into a usable format. This data is not in a clear format so our ability to upload it may be compromised."
  },
  {
    "objectID": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html",
    "href": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html",
    "title": "Thesis and Continuing Exploration",
    "section": "",
    "text": "To narrow down to certain states, we will focus on Massachusetts and Mississippi. Massachusetts is the most educated state in the US, and Mississippi is the least educated state in 2009, when our data starts."
  },
  {
    "objectID": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html#thesis",
    "href": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html#thesis",
    "title": "Thesis and Continuing Exploration",
    "section": "Thesis",
    "text": "Thesis\nMississippi, with its higher percentage of economically disadvantaged students, higher poverty rate, higher unemployment rate, lower percentage of parents holding bachelor’s degrees, and lower median income, has lower mean scores in both language and math for all students compared to Massachusetts."
  },
  {
    "objectID": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html#more-exploration-of-independent-variables",
    "href": "posts/2024-04-19-blog-post-6-thesis-and-continuing-exploration/blog-post-6-thesis-and-continuing-exploration.html#more-exploration-of-independent-variables",
    "title": "Thesis and Continuing Exploration",
    "section": "More Exploration of Independent Variables",
    "text": "More Exploration of Independent Variables\n\nMassachusetts\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\nMississippi\n\n\n\n\n\nAfter observing each plot for the variables, all variables are skewed except for the median income and the mean score. Therefore, we will apply natural log or square root transformations for those variables to deal with the skewness.\n\n\n\n\n\n\n\n\nSince some of the variables are still skewed, we will find more potential transformations or use a Robust Regression model that is less sensitive to outliers and skewness."
  },
  {
    "objectID": "posts/2024-04-10-blog-post-5-the-second-dataset/blog-post-5-the-second-dataset.html#our-secondary-dataset",
    "href": "posts/2024-04-10-blog-post-5-the-second-dataset/blog-post-5-the-second-dataset.html#our-secondary-dataset",
    "title": "The Second Dataset",
    "section": "Our Secondary Dataset",
    "text": "Our Secondary Dataset\nIn our ongoing quest to extract meaningful insights from educational data, we’ve recently enhanced our dataset with the inclusion of three pivotal new variables. Sourced from the Stanford Education Data Archive (SEDA), these additions—lninc50all, baplusall, and povertyall—represent, respectively, the natural logarithm of the median income across all datasets, the percentage of parents holding at least a Bachelor’s degree, and the overall poverty rate. This data enrichment is performed with the intention of capturing a more nuanced picture of educational disparities on a state level, year by year. To ensure the integrity of our analysis, meticulous steps were taken to filter out any instances of missing values, thus maintaining the robustness of our research framework.\nAs we stand, the datasets have been methodically merged, organizing the newly adopted variables in alignment with the existing structure based on state and temporal parameters. Preliminary scrutiny of the enhanced dataset reveals promising avenues for exploration. For instance, the incorporation of lninc50all allows us to investigate the elasticity of educational outcomes in relation to median income, while baplusall and povertyall offer a lens to examine the correlation between parental education, poverty levels, and educational equity. As we delve deeper into the data, our next steps will be geared towards applying advanced analytical models to discern patterns and potentially causal relationships. Stay tuned as we continue to unfold the narratives hidden within the numbers, with the ultimate aim of informing policies that bridge educational gaps.\nLink for the new dataset: https://edopportunity.org/get-the-data/seda-archive-downloads/"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#where-does-our-data-come-from",
    "href": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#where-does-our-data-come-from",
    "title": "Data Background",
    "section": "Where does our data come from?",
    "text": "Where does our data come from?\nOur data comes from the Stanford Education Data Archive, also known as SEDA. This is a project conducted by researches at the Stanford Graduate School of Education.The purpose of SEDA is to provide comprehensive, publicly available data on U.S. K-12 education. The data is primarily gathered for academic research, policy analysis, and accountability purposes. Researchers utilize it to study educational disparities, policy effectiveness, and the impact of various interventions. Policymakers rely on this data to make informed decisions, allocate resources, and hold stakeholders accountable for student outcomes. Despite persistent racial, socioeconomic, and gender disparities in academic performance and educational attainment within the U.S. educational system, SEDA emphasizes that these gaps are not inevitable or unchangeable.\nThe data is collected from various sources, including the U.S. Department of Education, state education agencies, and other publicly available datasets. It includes information on student demographics, achievement outcomes, funding, and other factors relevant to understanding educational disparities and opportunities across different regions and demographics. The advantage of SEDA is that it aggregates and organizes the data.\nSEDA combines information from the National Assessment of Educational Progress regarding the test scores in each school, geographic district, county, or state to compare scores from state tests on a common national level"
  },
  {
    "objectID": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#what-is-reported",
    "href": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#what-is-reported",
    "title": "Data Background",
    "section": "What is reported?",
    "text": "What is reported?\nThe raw data includes only counts of students scoring at different test-score levels, not individual test scores.This data includes constructed margin of errors for each of the measures of average test scores, learning rate, and trends in average scores with 95% confidence intervals. The data does not report average performance, learning, and/or trends if there were fewer than 20 students represented in the estimate, more than 30% of students in the unit took alternative assessments, or if the estimates are too imprecise to be informative."
  },
  {
    "objectID": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#what-issues-do-we-predict",
    "href": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#what-issues-do-we-predict",
    "title": "Data Background",
    "section": "What issues do we predict?",
    "text": "What issues do we predict?\nThere could be a possible bias in the learning-rate estimates towards districts with high-achieving kids moving out and low-achieving kids moving in, in the later years. Consequently, the data would be underestimating the learning rate in a school or district."
  },
  {
    "objectID": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#who-has-used-this-data",
    "href": "posts/2024-03-25-blog-post-2-data-background/blog-post-2-data-background.html#who-has-used-this-data",
    "title": "Data Background",
    "section": "Who has used this data?",
    "text": "Who has used this data?\nThe Stanford Education Data Archive (SEDA) has been instrumental in education policy and economics research, providing crucial insights into the educational impacts of COVID-19 through studies such as “The First Year of Pandemic Recovery: A District-Level Analysis” and “School District and Community Factors Associated With Learning Loss During the COVID-19 Pandemic”. These investigations delve into the significant shifts to remote learning and the associated declines in math and reading achievements, offering a district-level perspective on the pandemic’s profound effects on education. Moreover, by examining data from 7,800 districts, the research explores how income levels, minority status, and the extent of remote instruction have influenced learning losses, shedding light on the complex interplay of external factors affecting educational outcomes during this unprecedented time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Education Disparity in the US",
    "section": "",
    "text": "Polishing and Final Exploration\n\n\nBlog Post 7\n\n\nEditing previous work to make it presentable\n\n\n\n\n\n\nApr 24, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nThesis and Continuing Exploration\n\n\nBlog Post 6\n\n\nCreating our thesis that will be the basis of our project\n\n\n\n\n\n\nApr 19, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nThe Second Dataset\n\n\nBlog Post 5\n\n\nIdentifying a secondary dataset to use in our analysis\n\n\n\n\n\n\nApr 10, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Data\n\n\nBlog Post 4\n\n\nOur first steps of data analysis\n\n\n\n\n\n\nApr 8, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nEquity and Data Cleaning\n\n\nBlog Post 3\n\n\nDiscussing what we did to clean our data and whether it is equitable\n\n\n\n\n\n\nApr 1, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nData Background\n\n\nBlog Post 2\n\n\nDiscussing where our data comes from\n\n\n\n\n\n\nMar 25, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nPossible data sets\n\n\nBlog Post 1\n\n\nIdentifying possible datasets for our project\n\n\n\n\n\n\nMar 4, 2024\n\n\nGroup 10\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team Group 10. The members of this team are below."
  },
  {
    "objectID": "about.html#aruzhan-bektemirova",
    "href": "about.html#aruzhan-bektemirova",
    "title": "About",
    "section": "Aruzhan Bektemirova",
    "text": "Aruzhan Bektemirova\nAruzhan is an undergraduate student in Economics and Mathematics. https://github.com/aru-beka"
  },
  {
    "objectID": "about.html#chase-stephens",
    "href": "about.html#chase-stephens",
    "title": "About",
    "section": "Chase Stephens",
    "text": "Chase Stephens\nChase is an undergraduate student in Economics and Mathematics. https://github.com/chasedstephens"
  },
  {
    "objectID": "about.html#morgan-fleming",
    "href": "about.html#morgan-fleming",
    "title": "About",
    "section": "Morgan Fleming",
    "text": "Morgan Fleming\nMorgan is an undergraduate student in Economics and Mathematics. https://github.com/mlfleming"
  },
  {
    "objectID": "about.html#taelor-anderson",
    "href": "about.html#taelor-anderson",
    "title": "About",
    "section": "Taelor Anderson",
    "text": "Taelor Anderson\nTaelor is an undergraduate student in Psychology with a minor in Statistical Methods. https://github.com/taelorra"
  },
  {
    "objectID": "about.html#yixiao-li",
    "href": "about.html#yixiao-li",
    "title": "About",
    "section": "Yixiao Li",
    "text": "Yixiao Li\nLi is an undergraduate student in Economics and Mathematics. https://github.com/YixiaoLi0922\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "big_picture.html#introduction",
    "href": "big_picture.html#introduction",
    "title": "From Bay State to Magnolia State",
    "section": "Introduction",
    "text": "Introduction\nImagine two children, one in Massachusetts, the richest state in terms of educational attainment, and the other in Mississippi, ranked lowest on the same scale. Though separated by over a thousand miles, the distance is more than geographical—it’s educational, economic, and deeply rooted in the very fabric of society. This article dives into the complex interplay of socioeconomic factors that shape educational outcomes, revealing how deeply inequality is entrenched from one generation to the next."
  },
  {
    "objectID": "big_picture.html#thesis-statement",
    "href": "big_picture.html#thesis-statement",
    "title": "From Bay State to Magnolia State",
    "section": "Thesis Statement",
    "text": "Thesis Statement\nIn Massachusetts and Mississippi, the chasm in educational outcomes extends beyond mere geography and taps directly into profound socioeconomic disparities. Our analysis, focusing on district-level data, demonstrates a stark correlation: in areas where economic disadvantage is more pronounced, educational performance, particularly in language and math, significantly declines."
  },
  {
    "objectID": "big_picture.html#data-and-methodology",
    "href": "big_picture.html#data-and-methodology",
    "title": "From Bay State to Magnolia State",
    "section": "Data and Methodology",
    "text": "Data and Methodology\nOur study utilizes a comprehensive dataset encompassing district-level information from both states, including metrics on economically disadvantaged students, poverty rates, unemployment levels, median income, parental educational attainment, and racial demographics. By analyzing these variables, we aim to paint a detailed picture of how socioeconomic factors influence student performance across diverse communities.\n\nMassachusetts\n\n\n\n\n\n\n\nMississippi\n\n\n\n\n\nAs we journey through the educational terrain of Massachusetts and Mississippi, the contrasts are as stark as the shades of blue that paint our histograms. From the cobalt peaks representing Massachusetts’s highly educated parents to the navy troughs echoing Mississippi’s economic struggles, each bar tells a story.\nIn Massachusetts, the histogram showcasing parental education is a mountain range of ambition, with a majority of districts featuring a high percentage of parents holding bachelor’s degrees. It’s a testament to the state’s rich educational foundation. Contrast this with Mississippi, where these peaks are notably subdued, hinting at a landscape where higher education is a distant summit for many.\nEconomic disadvantage casts long shadows across the Mississippi graph, where the frequency of high percentages is all too common, a reflection of the hardship many students face before they even step into a classroom. Massachusetts, while not immune to these challenges, shows a distribution with fewer districts at the higher end of economic disadvantage, suggesting a different kind of starting line for students.\nThe racial composition histograms are a mosaic of demographic diversity. Massachusetts’s ‘White’ bars rise high, a sign of racial homogeneity in many districts. Mississippi tells a different story, with ‘Black’ significantly more represented, raising questions about how racial and economic factors interweave to shape educational outcomes.\nPoverty and unemployment in Mississippi seem to walk hand in hand, with histograms skewing towards higher rates than those in Massachusetts. It’s a silent narrative of the daily challenges students bring with them to school, burdens that can weigh heavily on academic achievement."
  },
  {
    "objectID": "big_picture.html#analysis",
    "href": "big_picture.html#analysis",
    "title": "From Bay State to Magnolia State",
    "section": "Analysis",
    "text": "Analysis\n\nEconomic Factors\nOur initial focus is on economic variables. Poverty and unemployment, often intertwined, emerge as significant predictors of educational success. Districts with higher poverty rates consistently show lower educational outcomes. This pattern underscores the critical role that economic stability plays in supporting academic achievement.\n\n\nEducational Background of Parents\nAnother striking factor is the educational attainment of parents. Districts where a higher percentage of parents hold bachelor’s degrees or higher often report better student performance. This relationship highlights the cyclical nature of education—where educational advantages and disadvantages are passed down through generations, potentially widening the gap.\n\n\nRacial Demographics\nFinally, the racial composition of districts also plays a critical role in educational outcomes. Our analysis indicates varied performance across different racial groups, with significant disparities evident between and within states. These findings provoke a broader discussion on the intersection of race, education, and socioeconomic status.\n\nOur exploration reveals a troubling yet clear picture: socioeconomic factors like poverty, unemployment, and parental education significantly influence educational outcomes in Massachusetts and Mississippi. This underscores the urgent need for policy interventions aimed at reducing educational disparities as a step towards greater social equity."
  },
  {
    "objectID": "big_picture.html#call-to-action",
    "href": "big_picture.html#call-to-action",
    "title": "From Bay State to Magnolia State",
    "section": "Call to Action",
    "text": "Call to Action\nUnderstanding these disparities is just the beginning. We must now turn insights into action. By supporting policies that target educational equity—such as improving access to quality education, enhancing parental support programs, and addressing systemic poverty—we can begin to close this educational gap. Let’s commit to making a difference, not just in Massachusetts and Mississippi, but across the nation."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "From Bay State to Magnolia State",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "From Bay State to Magnolia State",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "data.html#our-data",
    "href": "data.html#our-data",
    "title": "SEDA Data",
    "section": "Our Data",
    "text": "Our Data\nOur data comes from the Stanford Education Data Archive (SEDA). The purpose of SEDA is to provide comprehensive, publicly available data on U.S. K-12 education academic research, policy analysis, and accountability purposes. It is commonly sourced from for the study of educational disparities, policy effectiveness, and the impact of various interventions.\nThe data is primarily gathered for academic research, policy analysis, and accountability purposes. Researchers utilize it to study educational disparities, policy effectiveness, and the impact of various interventions. Policymakers rely on this data to make informed decisions, allocate resources, and hold stakeholders accountable for student outcomes. Link to the original data source: https://edopportunity.org/get-the-data/seda-archive-downloads/"
  },
  {
    "objectID": "data.html#data-files-and-variables",
    "href": "data.html#data-files-and-variables",
    "title": "SEDA Data",
    "section": "Data Files and Variables",
    "text": "Data Files and Variables\n\nDistrict: geographic school district\nSubject:\n\nRLA: Reading Language Arts\nMTH: Math\n\nMn_score: Average test scores\nNumstu: number of students\nPer: percentage of students in grade\nSubgroups:\n\nAll: all students\nAsn: Asian students\nBlk: Black students\nFem: female students\nHsp: Hispanic students\nMale: male students\nMtr: Multiracial students\nNec: Not economically disadvantaged\nWht: white students\n\nTotenrl: Number of students in grade\nUnempall: unemployment rate\nSnapall: Snap receipt rate\nSingle_momall: Single mother hh rate"
  },
  {
    "objectID": "data.html#cleaning",
    "href": "data.html#cleaning",
    "title": "SEDA Data",
    "section": "Cleaning",
    "text": "Cleaning\n\nWe cleaned our data before importing it, as it was 500mB. We kept the scores for each race and the locations, as we are most interested in those.\nWe downloaded the raw data and saved it in the data folder.\nUtilized the dplyr package to:\n\nSelect relevant attributes for the project.\nRemove unnecessary attributes.\nRemove missing attributes.\nRemove unusual values.\nUtilized the collapse package to collapse selected variables into their mean, creating one observation per year, district, class subject, and state combination. Previously it was expanded into grade levels, and finding the mean across those for each year allowed for compression of the very large original data set. The cleaned data was exported to the data folder.\n\nLink to load_and_clean_data.R file: loading script\n\n\nSummary of numerical variables\nTo begin, we wanted to make a summary table of general descriptive statistics for the races included in our data and the economically challenged students to see the variation between their test scores. This is not broken down by location yet, but we wanted to visualize the data better so we would know how to proceed.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n  mn_score_all     mn_score_asn     mn_score_blk     mn_score_ecd  \n Min.   :-3.735   Min.   :-2.92    Min.   :-3.40    Min.   :-3.55  \n 1st Qu.:-0.197   1st Qu.: 0.26    1st Qu.:-0.63    1st Qu.:-0.42  \n Median : 0.030   Median : 0.60    Median :-0.45    Median :-0.24  \n Mean   : 0.025   Mean   : 0.59    Mean   :-0.44    Mean   :-0.26  \n 3rd Qu.: 0.249   3rd Qu.: 0.93    3rd Qu.:-0.25    3rd Qu.:-0.08  \n Max.   : 2.865   Max.   : 2.79    Max.   : 1.16    Max.   : 1.24  \n NA's   :23236    NA's   :205143   NA's   :180838   NA's   :61961  \n\n\nBy examining these histograms, we visually analyze the distribution of test scores within each subgroup. This visualization helps us understand the spread, central tendency, and potential outliers within the test scores across different racial groups and economic statuses."
  }
]